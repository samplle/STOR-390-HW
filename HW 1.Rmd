---
title: "HW 1"
author: "Sam Pell"
date: "1/26/2024"
output:
  word_document: default
  html_document:
    number_sections: yes
  pdf_document: default
---

We have now seen three ethical theories as well as their pitfalls.  We have also thoroughly discussed their applications to the justifibility chat GPT.

# 

Now it is your turn to demonstrate your own understanding and mastery of these philosophical concepts.  To what extent, or under what circumstances, is the use of Chat-GPT justifiable in academic settings?  Use (at least) one of the above principles as a supporting argument for your position.  Argue for your position in one page or less. 

    Academic settings are *rife* with the use of Chat-GPT. Everything from papers to proofs to political statements can and will be written entirely by computers. This not only reduces the workload of the individuals using such tools, but also removes their focus, their mental stake in the product. People are creating things that even they do not fully grasp. Understanding this, it is essential to use generative AI with the **proper intent**. To do so, we must, then, create a set or sequence of usage policies for such a tool, to ensure that people are using it “the right way”.
    So what is the right way? While there are many ethical frameworks we can use to debate with, varying greatly in their perspectives on GPT, Immanuel Kant’s ideology, **deontology**, will be adept at solving this problem. In this framework, it is the moral *intention*, rather than the consequence, that matters. Within this base, we can state that any action which cannot be universally adapted and used is morally wrong. This includes dishonesty, which itself is self-defeating--if everyone lied all of the time, then no trust could build, and thus, lying would lose its very purpose. Furthermore, lying to someone, such as a professor that you are turning an assignment into, uses that person as a means to an end, rather than as an end in their own right. This is dehumanizing and further morally wrong. From this, though, adaptations must be made. We cannot ignore fault lines in the groundwork. 
    Rigidity is often an issue of note when discussing Kant’s deontology. In the famed example of a murderer knocking at the door, one is forced to choose between lying, being deceitful and treating the murderer as a means to an end, and knowingly giving up a currently-harbored future victim. Kant would argue that in the frameworks of pure deontology, we must give up the victim, as lying is morally wrong. However, such exceptions can be made in a diluted version of deontology, which is more adapted to the real world. As such, if such a pressing situation arises, it cannot be morally wrong to use Chat-GPT or other tools, if they should be able to make such a vast difference. The act of dishonesty can be justified *if* it prevents immediate and serious harm.
    When we use Chat-GPT, we are not creating our own work. Rather, we are being handed a completed work which has no human owner. GPT cannot contest the intellectual ownership of such a work. However, if such a piece of work is created, we also cannot *honestly* claim it is of our own making. I cannot venture to say what percentage of it we can account for, since we are dealing with an algorithm that does take our prompts, but uses an unseen, **black-box algorithm** to generate its output. Surely, then, to retain our honesty, we must disclose the usage of AI in our work. 
    So what is the final policy then? Can we use AI at all? **I think so.** AI is a helpful tool, and can often benefit people, but most especially when used with full honesty. We must, then, disclose our *full methodology* to those we may submit a paper to. We must give clear instructions on which AI was used, how it was used, what prompts we gave it, and any other relevant information. In cases where lives must be saved, and full disclosure may not be possible, sure, there is a moral justification for the prevention of immediate harm, but otherwise, it is essential to craft original works through honest and open use (or disuse) of AI. 

# 

In no more than half a page, consider and refute a potential objection to the position you staked out in the above question. 

    In considering potential objections to the deontologically-grounded argument above, I might raise the following moral quandary: suppose a student is not in any life-threatening situation. However, they have used AI on a final class project. This is work that they could not have performed otherwise, especially given the time constraints of academia. Without said work, they would not be able to graduate from their university on time. However, their professor does not allow the use of any AI on any work. Any violations of this rule would be condemned and brought before the university's honor court, which would then be likely to expel any student found guilty of violating such a code. 
    This student would, by the above logic, be *morally obligated* to admit to the violation, as not admitting to such an act would be an act of dishonesty. Furthermore, as the dishonesty does not prevent immediate harm to said student, they would not qualify for any moral exemptions. However, by being dishonest, the student would save themself from potential costs associated with delayed graduation, expulsion, and probable future income differentials. This could affect the remainder of the student's life, and could potentially have ramifications *decades* down the line.
    However, this *does not excuse* said student from their moral obligations. The student does not know how their teacher will react to their usage. They are not entirely certain of their expulsion. They cannot be certain that they will lose money in the long run, nor that this will not result in their best possible imagined outcome. Further, the student is not in serious or immediate danger. Rather, they are actively choosing to commit a moral wrong, in order to sate some inner idea of what path they must follow. Thus, the student is still morally bound to oblige the AI use policy defined above: complete and total honesty.